{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "lqJbK2a-sHdu",
      "metadata": {
        "id": "lqJbK2a-sHdu"
      },
      "source": [
        "## RandAugmentation_simple\n",
        "- baseline code가 존재하는 디렉토리에 해당 노트북을 다운받아 실행해주시기바랍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5919106d-53e4-4e07-ad0a-ef9e3a04d3b3",
      "metadata": {
        "id": "5919106d-53e4-4e07-ad0a-ef9e3a04d3b3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from typing import Tuple\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "if \"./\" not in sys.path:\n",
        "    sys.path.append(\"./\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ab232d-8273-4091-8fdf-fd946b1cdfa6",
      "metadata": {
        "id": "d0ab232d-8273-4091-8fdf-fd946b1cdfa6"
      },
      "outputs": [],
      "source": [
        "from src.model import Model\n",
        "from src.trainer import TorchTrainer\n",
        "from src.loss import CustomCriterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "830e4980-fbb3-4113-9537-868b21003650",
      "metadata": {
        "id": "830e4980-fbb3-4113-9537-868b21003650"
      },
      "outputs": [],
      "source": [
        "MEAN_V = (0.4914, 0.4822, 0.4465)\n",
        "STD_V = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "DATASET_DIR = \"./input/cifar10\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf58e84-3317-4311-b9bb-b1ae570b62f7",
      "metadata": {
        "id": "8bf58e84-3317-4311-b9bb-b1ae570b62f7"
      },
      "outputs": [],
      "source": [
        "def generate_transform(resize: int = 32, aug_fcns: Tuple = ()) -> transforms.transforms.Compose:\n",
        "    \"\"\"Generate train augmentation policy.\"\"\"\n",
        "    transform_fcns = []\n",
        "    transform_fcns.append(transforms.Resize((resize, resize)))\n",
        "    transform_fcns += list(aug_fcns)\n",
        "    \n",
        "    transform_fcns.append(transforms.ToTensor())\n",
        "    transform_fcns.append(transforms.Normalize(MEAN_V, STD_V))\n",
        "    \n",
        "    return transforms.Compose(transform_fcns)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b126f56c-212b-4512-b468-5ebc63eb3bc4",
      "metadata": {
        "id": "b126f56c-212b-4512-b468-5ebc63eb3bc4"
      },
      "outputs": [],
      "source": [
        "def load_cifar10(img_size: int = 32, \n",
        "                 aug_fcns: Tuple = (), \n",
        "                 validation_ratio: float = 0.8,\n",
        "                ) -> Tuple[CIFAR10, CIFAR10, CIFAR10]:\n",
        "    tf_train = generate_transform(resize=img_size, aug_fcns=aug_fcns)\n",
        "    tf_test = generate_transform(resize=img_size)\n",
        "    \n",
        "    train_dataset = CIFAR10(root=DATASET_DIR, train=True, download=True, transform=tf_train)\n",
        "    train_length = int(len(train_dataset) * validation_ratio)\n",
        "    val_length = len(train_dataset) - train_length\n",
        "\n",
        "    train_dataset, val_dataset = random_split(train_dataset, [train_length, val_length])\n",
        "    test_dataset = CIFAR10(root=DATASET_DIR, train=False, download=True, transform=tf_test)\n",
        "    \n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5076e220-769e-4e05-896a-5caabb17ebca",
      "metadata": {
        "id": "5076e220-769e-4e05-896a-5caabb17ebca"
      },
      "outputs": [],
      "source": [
        "def tensor_to_img(tensor_img: torch.Tensor) -> np.ndarray:\n",
        "    return ((tensor_img.permute(1, 2, 0).numpy() * STD_V + MEAN_V) * 255).astype(np.uint8)\n",
        "\n",
        "def visualize_datasets(_train_dataset: CIFAR10, _val_dataset: CIFAR10, _test_dataset: CIFAR10, title_prefix: str = \"\") -> None:\n",
        "    fig, ax = plt.subplots(3, 7, figsize=(20, 10))\n",
        "\n",
        "    for i in range(7):\n",
        "        idx = np.random.randint(0, len(_val_dataset))\n",
        "\n",
        "        ax[0][i].imshow(tensor_to_img(_train_dataset[idx][0]))\n",
        "        ax[1][i].imshow(tensor_to_img(_val_dataset[idx][0]))\n",
        "        ax[2][i].imshow(tensor_to_img(_test_dataset[idx][0]))\n",
        "\n",
        "        ax[0][i].axis('off')\n",
        "        ax[1][i].axis('off')\n",
        "        ax[2][i].axis('off')\n",
        "\n",
        "    fig.suptitle(f\"{title_prefix} Visualization of Augmentation.\\n(Each row represents train, validation, test dataset accordingly)\")\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "051fde3e-1319-468f-bb3c-9fc2003be1b6",
      "metadata": {
        "id": "051fde3e-1319-468f-bb3c-9fc2003be1b6"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ea21cf-62be-4105-83ce-9366863e5030",
      "metadata": {
        "id": "d3ea21cf-62be-4105-83ce-9366863e5030"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 256\n",
        "    \n",
        "def objective(trial: optuna.Trial) -> float:\n",
        "    img_size = 32\n",
        "\n",
        "    augmentation_functions = []\n",
        "    use_color_jitter = trial.suggest_categorical(\"aug_color_jitter\", [True, False])\n",
        "    use_random_perspective = trial.suggest_categorical(\"aug_random_perspective\", [True, False])\n",
        "    use_random_flip = trial.suggest_categorical(\"aug_random_flip\", [True, False])\n",
        "    \n",
        "    if use_color_jitter:\n",
        "        augmentation_functions.append(transforms.ColorJitter(brightness=(0.5, 1.5), \n",
        "                                                             contrast=(0.5, 1.5), \n",
        "                                                             saturation=(0.5, 1.5)))\n",
        "    if use_random_perspective:\n",
        "        augmentation_functions.append(transforms.RandomPerspective())\n",
        "    \n",
        "    if use_random_flip:\n",
        "        augmentation_functions.append(transforms.RandomHorizontalFlip())\n",
        "    \n",
        "        \n",
        "    train_dataset, val_dataset, test_dataset = load_cifar10(img_size=img_size, \n",
        "                                                            aug_fcns = tuple(augmentation_functions), \n",
        "                                                            validation_ratio=0.8)\n",
        "    \n",
        "    visualize_datasets(train_dataset, val_dataset, test_dataset, title_prefix=f\"Trial {trial.number:03d} //\")\n",
        "    plt.draw()\n",
        "    plt.show()\n",
        "    print(augmentation_functions)\n",
        "    \n",
        "    with open(\"./configs/model/mobilenetv3.yaml\", \"r\") as f:\n",
        "        model_cfg = yaml.load(f, yaml.SafeLoader)\n",
        "\n",
        "    model_cfg['backbone'][-1][-1] = [10]\n",
        "\n",
        "    model = Model(model_cfg, verbose=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.model.parameters(), lr=0.1, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr=0.1, steps_per_epoch=len(train_dataset), epochs=EPOCHS, pct_start=0.05)\n",
        "    criterion = CustomCriterion(samples_per_cls=None, device=device)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, \n",
        "                              pin_memory=torch.cuda.is_available(), \n",
        "                              shuffle=True, \n",
        "                              batch_size=BATCH_SIZE, \n",
        "                              num_workers=4, \n",
        "                              drop_last=True)\n",
        "    val_loader = DataLoader(dataset=val_dataset, \n",
        "                            pin_memory=torch.cuda.is_available(), \n",
        "                            shuffle=False, \n",
        "                            batch_size=BATCH_SIZE, \n",
        "                            num_workers=4)\n",
        "    test_loader = DataLoader(dataset=test_dataset, \n",
        "                             pin_memory=torch.cuda.is_available(), \n",
        "                             shuffle=False, \n",
        "                             batch_size=BATCH_SIZE, \n",
        "                             num_workers=4)\n",
        "\n",
        "    exp_dir = \"./exp/autoaug\"\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "    trainer = TorchTrainer(model=model, \n",
        "                           criterion=criterion, \n",
        "                           optimizer=optimizer, \n",
        "                           scheduler=scheduler, \n",
        "                           device=device, \n",
        "                           verbose=1, \n",
        "                           model_path=os.path.join(exp_dir, \"best.pt\"))\n",
        "\n",
        "    best_acc, best_f1 = trainer.train(train_dataloader=train_loader, \n",
        "                                      n_epoch=EPOCHS, \n",
        "                                      val_dataloader=val_loader)\n",
        "    print(\"TEST DATASET\")\n",
        "    test_loss, test_f1, test_accuracy = trainer.test(model, test_loader)\n",
        "    \n",
        "    return test_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e604dbf5-e4d9-433f-97f3-29523d65208a",
      "metadata": {
        "id": "e604dbf5-e4d9-433f-97f3-29523d65208a"
      },
      "outputs": [],
      "source": [
        "wandb.init()\n",
        "study = optuna.create_study(direction=\"maximize\", study_name=\"autoaug\", load_if_exists=True)\n",
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61df5122-ef44-40e1-84f9-ca517524b445",
      "metadata": {
        "id": "61df5122-ef44-40e1-84f9-ca517524b445"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RandAugmentation_simple.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "e31c68abf1d5dd3f9e2269f23eadf1b199587e56c0618a30760176a65ebfcab4"
    },
    "kernelspec": {
      "display_name": "lightweight",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
